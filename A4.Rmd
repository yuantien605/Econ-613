---
title: "A4"
author: "Yuan Tien"
date: "4/9/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
rm(list = ls())
getwd()
setwd("/Users/yuantien/Desktop/R/613/A4")
library(tidyverse)
library(data.table)

dat <- fread("dat_A4.csv")
```

Exercise I 

1.1
To my understanding, the latest interview is in 2019, so we compute age base on 2019 - birth year 
```{r}
dat97 <- dat %>%
  mutate(age = 2019 - KEY_BDATE_Y_1997)
dat97$expweek <- rowSums(dat97[,18:28], na.rm = T)

#Since a year has 52.1429 weeks approximately, I will divide weeks by this number
dat97$work_exp <- dat97$expweek/52.1429
```

1.2
Use YSCH-3113 to compute

GED is equivalent to 12th grade (12 years of schooling)
see:
http://usgei.org/high-school-equivalency-ged/#:~:text=GEI%20offers%20international%20students%20the,globally%20have%20earned%20GED%20diplomas.

I assume degree earned = None is 0 years of schooling
Associate degree and junior college normally takes two years. I assume they take 12+2 = 14 years of schooling 
Assume normal college degree takes 4 years (12+4 = 16) 
I assume master's degree takes additional 2 years out of college (12+4+2), and PhD takes 4 years out of college (12+ 4 +4) =20

I assume DDS, JD and MD takes 4 years out of college (12+4+4 = 20)

```{r}
s = dat97$YSCH.3113_2019
news <- recode(s, '1' = 0, "2" = 12, "3" = 12, "4" = 14, "5" = 16, "6" = 18 , "7" = 20, "8" = 20, "-1" = 0, "-2" = 0) #here I assume those refuse to answer and those who don't know their schooling as 0 year of schooling.
dat97$schooling = news

dat97$biodad_school <- dat97$CV_HGC_BIO_DAD_1997
dat97$biomom_school <- dat97$CV_HGC_BIO_MOM_1997
dat97$resdad_school <- dat97$CV_HGC_RES_DAD_1997
dat97$resmom_school <- dat97$CV_HGC_RES_MOM_1997

#I leave "Ungraded = 95" as it be for now

# dat97$biodad_school <- recode(dat97$biodad_school, "95" = 0)
# dat97$biomom_school <- recode(dat97$biomom_school, "95" = 0)
# dat97$resdad_school <- recode(dat97$resdad_school, "95" = 0)
# dat97$resmom_school <- recode(dat97$resmom_school, "95" = 0)
```

1.3.1
```{r}
#Income distribution by age group
dat97%>%
  filter(YINC_1700_2019 > 0) %>%
  ggplot(aes(x = YINC_1700_2019, color = as.factor(age))) + geom_density() 

dat97%>%
  filter(YINC_1700_2019 > 0) %>%
  ggplot(aes(x = as.factor(age), y = YINC_1700_2019, )) + geom_boxplot()
```


```{r}
dat97%>%
  filter(YINC_1700_2019 > 0) %>%
  ggplot(aes(x = YINC_1700_2019, color = as.factor(KEY_SEX_1997))) + geom_density() + labs(color = "1 = Male")

dat97%>%
  filter(YINC_1700_2019 > 0) %>%
  ggplot(aes(x = as.factor(KEY_SEX_1997), y = YINC_1700_2019)) + geom_boxplot() + xlab("1 = Male") +ylab("Income")

```

```{r}
dat97%>%
  filter(YINC_1700_2019 > 0) %>%
  filter(CV_BIO_CHILD_HH_U18_2019 >= 0) %>%
  ggplot(aes(x = as.factor(CV_BIO_CHILD_HH_U18_2019), y = YINC_1700_2019)) + geom_boxplot() + xlab("Number of children")+ylab("Income") + labs(subtitle = "Since there is nearly no obs with 8 or 9 children, the boxplots behave like this")

```

1.3.2
Income Age
```{r}
income_age <- as.data.frame.matrix(table(dat97$age, dat97$YINC_1700_2019))  
income_age$share_of_zero <- (income_age[,1])/rowSums(income_age)

tibble(age = 35:39, share_of_zero = income_age$share_of_zero)
```

Gender Income
```{r}
  income_gender <- as.data.frame.matrix(table(dat97$KEY_SEX_1997, dat97$YINC_1700_2019))  
  income_gender$share_of_zero <- (income_gender[,1])/rowSums(income_gender)

 tibble(sex = c("male", "female"), share_of_zero = income_gender$share_of_zero)
```

```{r}
income_child <- as.data.frame.matrix(table(dat97$CV_BIO_CHILD_HH_U18_2019, dat97$YINC_1700_2019))  
income_child$share_of_zero <- (income_child[,1])/rowSums(income_child)

tibble(number_of_children = 0:9, share_of_zero = income_child$share_of_zero)
```


```{r}
marry_income <- as.data.frame.matrix(table(dat97$CV_MARSTAT_COLLAPSED_2019, dat97$YINC_1700_2019))  
marry_income$share_of_zero <- (marry_income[,1])/rowSums(marry_income)

tibble(marital_status = c("Never_married", "Married", "Separated", "Divorced", "Widowed"), share_of_zero = marry_income$share_of_zero)
```
1.3.3

Interpretation

1. The age of the respondents seems not to be associated with income.
2. Male appears to earn more than female.
3. People with small family size (i.e, 1-3 children) seems to earn more than people with no children and more than 3 children.
4. 35 & 38 years old age group has more people with 0 income than other groups in proportion.
5. There are more male with 0 income than female in proportion.
6. People with less children has a larger share of people with 0 income.


Exercise II

2.1
```{r}
#Proposed model: income_i = b0+ b1 * work_experience_i + b2 * schooling_i

dat97 %>%
  filter(YINC_1700_2019 >0) %>%
  lm(YINC_1700_2019 ~ work_exp + schooling, data =.) %>%
  summary()

```

This OLS model, however, may suffer from selection problem since people who report 0 income or those that don't report income is not random. There might be ommitted variable that influence the reporting bias and also work experience and years of schooling.


Hence, we could consider Heckman's two step estimation.

2.2

Heckman's Two-Step estimator consider the selection problem by estimating the part that determines the dependent variable (here is income) but is not our explanatory variable. Using that part as a regressor for a second stage regression, we can obtain consistent estimates of x ruling out the effect of selection.

2.3 

Hung-Wei said on Slack that we can use glm() to estimate the first stage probit
```{r}
#create a dummy for income >0 

dat97$nonmiss <- ifelse(dat97$YINC_1700_2019 > 0, 1,0)

dat97$nonmiss[is.na( dat97$nonmiss ) == T] = 0 #make missing value = 0 

first <- glm(formula =  nonmiss ~ work_exp + schooling, family = binomial(link = "probit"), data = dat97)
summary(first)

first_predict <- - predict(first) #remember a negative sign

inmills <- dnorm(first_predict)/ (1- pnorm(first_predict)) #pdf/cdf
summary(inmills)

Hecloglik <- function (par, work_exp, schooling, inmills)  {
  XB   = par[1] + par[2]* work_exp + par[3]* schooling + par[4] * inmills
  Prob = dnorm(XB)
  Prob[Prob>0.999999] = 0.999999 
  Prob[Prob<0.000001] = 0.000001
  Like = log(Prob)
  return( - sum(Like) )
}
```

Use lm() to help me find good starting value
```{r}
regdat <- dat97 %>%
  filter(is.na(work_exp) == F, is.na(schooling) == F) %>%
  cbind(inmills)

cheat <- lm(YINC_1700_2019 ~ work_exp + schooling + inmills, data = regdat)
as.vector(cheat$coefficients) 

startv <- as.vector(cheat$coefficients) 
#noisestartv <- jitter(startv) #add noise
#noisestartv

work_exp = regdat$work_exp
schooling = regdat$schooling
inmills = regdat$inmills

```

```{r}
results_2stage <- optim(startv, fn = Hecloglik, method = "BFGS", 
                  control = list(trace = 6, maxit = 3000),
                  work_exp = work_exp, schooling = schooling, inmills = inmills)
results_2stage$par
```
The results change a lot. Perhaps those that earn very little or unemployed are systematically absent from the survey, which creates overestimation using OLS.


Exercise 3
3.1
```{r}
dat97%>%
  filter(YINC_1700_2019 > 0) %>%
  ggplot(aes(x = YINC_1700_2019)) + geom_density() 

#Income should be top-coded at 100,000
```

3.2 & 3.3

I propose the two stage sample selection model to deal with censoring problem. I first explain top-coded incidents and then use the inverse mills ratio for the second stage estimation. 

Since glm & lm are allowed for two-stage test, I use them again here.

```{r}
#First stage: explaining top-coded income

dat97$topcode <- ifelse(dat97$YINC_1700_2019 == 100000, 1,0)

sum(is.na( dat97$topcode) ) #3572 NA values

datex3 <- dat97 %>%
  dplyr::select(topcode, YINC_1700_2019, work_exp, schooling) %>%
  filter(is.na(topcode) == F, is.na(work_exp) == F, is.na(schooling) == F)

#clear NA before going in estimation

topfirst <- glm(formula =  topcode ~ work_exp + schooling, family = binomial(link = "probit"), data = datex3)
summary(topfirst)

top_predict <- - predict(topfirst) #remember a negative sign

topmills <- dnorm(top_predict)/ (1- pnorm(top_predict)) #pdf/cdf
summary(topmills)

datex3 <- cbind(datex3, topmills)
#2nd stage

topsec<- lm(YINC_1700_2019 ~ work_exp + schooling + topmills, data = datex3)
summary(topsec)

```

3.4 
```{r}
#Results indicate that work experience and schooling are positively correlated with income as expected. 

ols <-  lm(YINC_1700_2019 ~ work_exp + schooling, data = datex3)
summary(ols)

#The effect size is smaller when we ignore censoring issue. That is, we may underestimate the effect of education and work experience if we just use the censor data.

```

Exercise 4

Goal: the effect of education, marital status, experience and education on wages

```{r}
list.files()
library(data.table)
library(tidyverse)
dat2 <- fread("dat_A4_panel.csv")

glimpse(dat2)
#indidual's panel information is on the columns

panel <- dat2 #just in case
```

4.1

The association between education and wage, marital status and wage, and experience and wage could have ability selection problem. People with better ability could be encouraged to study longer, and they could also earn more because of their productivity. Likewise, gifted people could do better in marriage market, and they could also work more years because of their ability. 

4.2

Prepare the data for Between and Within Estimator

mutate mean income by individual, mutate mean independent variable by individual
```{r}

str_subset( colnames(panel), "YINC") 
str_subset( colnames(panel), "DEGREE") 

panel <- panel %>%
  rowwise() %>%
  mutate(mincome = sum(c (`YINC-1700_1997` + `YINC-1700_1998` + `YINC-1700_1999` + `YINC-1700_2000` + `YINC-1700_2001` + 
         `YINC-1700_2002`+ `YINC-1700_2003` + `YINC-1700_2004`+ `YINC-1700_2005`+ `YINC-1700_2006`+ `YINC-1700_2007`+
         `YINC-1700_2008`+ `YINC-1700_2009`+  `YINC-1700_2010`+ `YINC-1700_2011` + `YINC-1700_2013`+ `YINC-1700_2015`+ 
         `YINC-1700_2017`+ `YINC-1700_2019`), na.rm =T) /19 ) %>%
  ungroup() %>%
  mutate( across(starts_with("CV_HIGHEST_DEGREE"), recode, "0" = 0, "1" = 12, "2" = 12, "3" = 14, "4" = 16, "5" = 18, "6"=  20 , "7" = 20, "-1" = 0, "-2" = 0, "-3" = 0)) %>%   #here I assume years of education for "Invalid Skip" = 0
  select(- c (CV_HIGHEST_DEGREE_EVER_EDT_2010, CV_HIGHEST_DEGREE_EVER_EDT_2011, CV_HIGHEST_DEGREE_EVER_EDT_2013)) 
  #drop these two columns to remain consistent with 19 years 19 columns

#checking
panel %>%
  select(starts_with("CV_HIGHEST_DEGREE")) %>%
  glimpse()

panel$mschool <- panel %>%
  select(starts_with("CV_HIGHEST_DEGREE")) %>%
  rowMeans(na.rm = T) 

#Now to deal with marital status

p2 <- panel #just in case

p2 <- p2 %>%
  mutate( across(starts_with("CV_MARSTAT"), recode, "0" = 0, "1" = 1, "2" = 0, "3" = 0, "4" = 0, "-1" = 0, "-2" = 0)) 
#Here I treat Separated Divorced and Widowed as not married, along side "never married"

p2$mmar <- p2 %>%
  select(starts_with("CV_MARSTAT")) %>%
  rowMeans(na.rm = T) 

# Finally, I have to deal with work experience

str_subset( colnames(p2), "CV_WKSWK_JOB" ) 

#exper <- function(x, p2) {
#  p2[, paste0("work_exp_", "x")] <- p2 %>%
#  select(starts_with("CV_WKSWK_JOB") & ends_with("x")) %>%
#  rowSums(na.rm = T)/52.1429 #compute experience in years
#}

p2$work_exp_1997 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("1997")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_1998 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("1998")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_1999 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("1999")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2000 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2000")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2001 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2001")) %>% rowSums(na.rm = T)/52.1429 

p2$work_exp_2002 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2002")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2003 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2003")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2004 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2004")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2005 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2005")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2006 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2006")) %>% rowSums(na.rm = T)/52.1429

p2$work_exp_2007 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2007")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2008 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2008")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2009 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2009")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2010 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2010")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2011 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2011")) %>% rowSums(na.rm = T)/52.1429

p2$work_exp_2013 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2013")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2015 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2015")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2017 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2017")) %>% rowSums(na.rm = T)/52.1429 
p2$work_exp_2019 <- p2 %>% select(starts_with("CV_WKSWK_JOB") & ends_with("2019")) %>% rowSums(na.rm = T)/52.1429 

str_which( colnames(p2), "work_exp" ) 

p2$mexp <- rowMeans(p2[, 250:268], na.rm = T) #column 252:270 are work_exp_1997 - work_exp_2019

```
Between Estimator

Note that I first deal with between estimator since I already got the means

```{r}
between <- p2 %>%
  select(mincome, mschool, mmar, mexp) %>%
  na.omit() %>%
  lm(mincome ~ mmar + mexp + mschool, data =.) 

summary(between)

```

Preparing data for within estimator
idea: yit - mean(yi) = beta * (xit - mean(xi))

```{r}
str_subset( colnames(p2), "work_exp") 
str_subset( colnames(p2), "MAR") 
str_subset( colnames(p2), "DEGREE") #note that we only have 18 rounds of education information (no such data for 1997)

p3 <- p2 #just in case

p3 <- rename(p3, DEGREE_2015 = CV_HIGHEST_DEGREE_EVER_EDT_2015, DEGREE_2017 = CV_HIGHEST_DEGREE_EVER_EDT_2017, DEGREE_2019 = CV_HIGHEST_DEGREE_EVER_EDT_2019)

p3 <- rename(p3, DEGREE_1998 = CV_HIGHEST_DEGREE_9899_1998, DEGREE_1999 = CV_HIGHEST_DEGREE_9900_1999,
             DEGREE_2000 = CV_HIGHEST_DEGREE_0001_2000, DEGREE_2001 = CV_HIGHEST_DEGREE_0102_2001,
             DEGREE_2002 = CV_HIGHEST_DEGREE_0203_2002, DEGREE_2003 = CV_HIGHEST_DEGREE_0304_2003,
             DEGREE_2004 = CV_HIGHEST_DEGREE_0405_2004, DEGREE_2005 = CV_HIGHEST_DEGREE_0506_2005,
             DEGREE_2006 = CV_HIGHEST_DEGREE_0607_2006, DEGREE_2007 = CV_HIGHEST_DEGREE_0708_2007,
             DEGREE_2008 = CV_HIGHEST_DEGREE_0809_2008, DEGREE_2009 = CV_HIGHEST_DEGREE_0910_2009,
             DEGREE_2010 = CV_HIGHEST_DEGREE_1011_2010, DEGREE_2011 = CV_HIGHEST_DEGREE_1112_2011,
             DEGREE_2013 = CV_HIGHEST_DEGREE_1314_2013)

install.packages("panelr")
library(panelr)

str_which( colnames(p3), "DEGREE")

str_which( colnames(p3), "MAR")

var_list = c(1,2, 247:249, 269, 250:268, str_which( colnames(p3), "YINC"), str_which( colnames(p3), "MAR"),str_which( colnames(p3), "DEGREE"))

try <- p3[,var_list]
try <- as.data.frame(try)

glimpse(try)

longp <- long_panel(try, label_location = "end", prefix = "_", periods = c(1997:2011, 2013, 2015, 2017, 2019))

saveRDS(longp, file = "longp.rds")

#In mutate, NA - some value will produce NA. That is fine for this analysis.

longp <- longp %>%
  mutate(income_diff = `YINC-1700` - mincome, mar_diff = CV_MARSTAT_COLLAPSED - mmar, 
         exp_dif = work_exp - mexp, sch_dif = DEGREE - mschool)
```

Within Estimator
```{r}
modwithin <- longp %>%
  select(income_diff, mar_diff, exp_dif, sch_dif) %>%
  na.omit() %>%
  lm(income_diff ~ mar_diff + exp_dif + sch_dif -1, data =.)

#Note that I did not include intercept because the intercept is subtracted by construction

summary(modwithin)

```

(First) Difference

Use long data
group by ID, lag value

Note that my first difference method is using yit minus its previous period. For a person i's data in 2009, I am taking the difference of 2009 and 2008.  

```{r}
longp <- longp %>%
  group_by(id) %>%
  mutate(lincome = dplyr::lag(`YINC-1700`, n = 1), lmar = dplyr::lag(CV_MARSTAT_COLLAPSED, n = 1), 
         lexp = dplyr::lag(work_exp, n =1), lsch = dplyr::lag(DEGREE, n = 1)) %>%
  mutate(fdincome = `YINC-1700` - lincome, fdmar = CV_MARSTAT_COLLAPSED - lmar, 
         fdexp = work_exp - lexp, fdsch = DEGREE - lsch)

modFD <- longp %>%
  select(fdincome, fdmar, fdexp, fdsch) %>%
  na.omit() %>%
  lm(fdincome ~ fdmar + fdexp + fdsch -1 , data =.)

#I also exclude the intercept since the intercept is subtracted with the first difference method
summary(modFD)
```

4.3

```{r}
coef <- list( First_difference = modFD$coefficients, Within = modwithin$coefficients, Between = between$coefficients)
coef
```

It is evident that my three models produce very different results. However, the coefficients for three models are all positive and significant. At least we can be certain about the positive relationship between our independent variables and income. 

The difference in coefficients might very well be how the models deal with NA. Because I did not drop "every" rows with NA value, the three approaches should have different observations going into the regressions. For example, the between estimator could have the most observations since I compute the mean regardless of the presence of NA in a row (I just skip that one with na.rm = T).

On the other hand, the first difference approach certainly won't use the observations in the first year (1997) while the within estimator and between estimator (incorporate in the mean value) will use them. 




